{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ebcc064",
   "metadata": {},
   "source": [
    "### Load energy and coordinates data from txt and save it as .h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "845a4b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import torchani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "161676ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_e = []\n",
    "data_xyz = []\n",
    "with open('DFT_CO.txt') as f:\n",
    "    for line in f:\n",
    "        data_e.append(float(line.strip('\\n')))\n",
    "with open('DFT_CO_xyz.txt') as f:\n",
    "    for line in f:\n",
    "        data_xyz.append(float(line.strip('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c4d0247",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = torchani.data._pyanitools.datapacker('./CO.h5', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8340e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = [str('C'),str('O')]\n",
    "coordinates = []\n",
    "energies = []\n",
    "for i in range(len(data_e)):\n",
    "    coordinates.append(np.array([[[0.0, 0.0, 0.0],\n",
    "                                [0.0, 0.0, data_xyz[i]]]]))\n",
    "    energies.append(np.array([data_e[i], ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72668877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(species[0],str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4bf3c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.store_data('CO', coordinates=coordinates, energies=energies, species=species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23bab146",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dfd68f",
   "metadata": {},
   "source": [
    "### Use torchani interface to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5dd668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchani\n",
    "import os\n",
    "import math\n",
    "import torch.utils.tensorboard\n",
    "import tqdm\n",
    "\n",
    "# helper function to convert energy unit from Hartree to kcal/mol\n",
    "from torchani.units import hartree2kcalmol\n",
    "\n",
    "# device to run the training\n",
    "device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dce407ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rcr = 5.2000e+00\n",
    "Rca = 3.5000e+00\n",
    "EtaR = torch.tensor([1.6000000e+01], device=device)\n",
    "ShfR = torch.tensor([9.0000000e-01, 1.1687500e+00, 1.4375000e+00, 1.7062500e+00, 1.9750000e+00, 2.2437500e+00, 2.5125000e+00, 2.7812500e+00, 3.0500000e+00, 3.3187500e+00, 3.5875000e+00, 3.8562500e+00, 4.1250000e+00, 4.3937500e+00, 4.6625000e+00, 4.9312500e+00], device=device)\n",
    "Zeta = torch.tensor([3.2000000e+01], device=device)\n",
    "ShfZ = torch.tensor([1.9634954e-01, 5.8904862e-01, 9.8174770e-01, 1.3744468e+00, 1.7671459e+00, 2.1598449e+00, 2.5525440e+00, 2.9452431e+00], device=device)\n",
    "EtaA = torch.tensor([8.0000000e+00], device=device)\n",
    "ShfA = torch.tensor([9.0000000e-01, 1.5500000e+00, 2.2000000e+00, 2.8500000e+00], device=device)\n",
    "species_order = ['H', 'C', 'N', 'O']\n",
    "num_species = len(species_order)\n",
    "aev_computer = torchani.AEVComputer(Rcr, Rca, EtaR, ShfR, EtaA, Zeta, ShfA, ShfZ, num_species)\n",
    "energy_shifter = torchani.utils.EnergyShifter(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b62a6c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading /home/micha/H2O2/my_training_H2O2/./group_test.h5, total molecules: 1\n",
      "1/1  [==============================] - 0.0s\n",
      "2/1  [============================================================] - 0.0s=> loading /home/micha/H2O2/my_training_H2O2/./group_test.h5, total molecules: 1\n",
      "1/1  [==============================] - 0.0s\n",
      "2/1  [============================================================] - 0.0sSelf atomic energies:  tensor([118.9049, -25.0879], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    path = os.path.dirname(os.path.realpath(__file__))\n",
    "except NameError:\n",
    "    path = os.getcwd()\n",
    "#dspath = os.path.join(path, 'CO.h5')\n",
    "dspath = os.path.join(path, './group_test.h5')\n",
    "#dspath = '../../Basics_Pytorch/torchani/examples/My_training_CO/group_test.h5'\n",
    "batch_size = 12#2560\n",
    "\n",
    "training, validation = torchani.data.load(dspath).subtract_self_energies(energy_shifter, species_order).species_to_indices(species_order).shuffle().split(0.8, None)\n",
    "training = training.collate(batch_size).cache()\n",
    "validation = validation.collate(batch_size).cache()\n",
    "print('Self atomic energies: ', energy_shifter.self_energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8ca49d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(torchani.data.load(dspath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476244eb",
   "metadata": {},
   "source": [
    "When iterating the dataset, we will get a dict of name->property mapping\n",
    "\n",
    "##############################################################################\n",
    "\n",
    " Now let's define atomic neural networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88e661a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aev_dim = aev_computer.aev_length\n",
    "\n",
    "H_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(aev_dim, 160),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(160, 128),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(128, 96),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(96, 1)\n",
    ")\n",
    "\n",
    "C_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(aev_dim, 144),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(144, 112),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(112, 96),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(96, 1)\n",
    ")\n",
    "# C_network = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(aev_dim, 112),\n",
    "#     torch.nn.CELU(0.1),\n",
    "#     torch.nn.Linear(112, 96),\n",
    "#     torch.nn.CELU(0.1),\n",
    "#     torch.nn.Linear(96, 1)\n",
    "# )\n",
    "\n",
    "N_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(aev_dim, 128),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(128, 112),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(112, 96),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(96, 1)\n",
    ")\n",
    "\n",
    "O_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(aev_dim, 128),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(128, 112),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(112, 96),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(96, 1)\n",
    ")\n",
    "# O_network = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(aev_dim, 112),\n",
    "#     torch.nn.CELU(0.1),\n",
    "#     torch.nn.Linear(112, 96),\n",
    "#     torch.nn.CELU(0.1),\n",
    "#     torch.nn.Linear(96, 1)\n",
    "# )\n",
    "\n",
    "nn = torchani.ANIModel([H_network, C_network, N_network, O_network])\n",
    "# nn = torchani.ANIModel([C_network, O_network])\n",
    "#print(nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce874e7b",
   "metadata": {},
   "source": [
    "Initialize the weights and biases.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Pytorch default initialization for the weights and biases in linear layers\n",
    "  is Kaiming uniform. See: `TORCH.NN.MODULES.LINEAR`_\n",
    "  We initialize the weights similarly but from the normal distribution.\n",
    "  The biases were initialized to zero.</p></div>\n",
    "\n",
    "  https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5071d8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ANIModel(\n",
       "  (0): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=160, bias=True)\n",
       "    (1): CELU(alpha=0.1)\n",
       "    (2): Linear(in_features=160, out_features=128, bias=True)\n",
       "    (3): CELU(alpha=0.1)\n",
       "    (4): Linear(in_features=128, out_features=96, bias=True)\n",
       "    (5): CELU(alpha=0.1)\n",
       "    (6): Linear(in_features=96, out_features=1, bias=True)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=144, bias=True)\n",
       "    (1): CELU(alpha=0.1)\n",
       "    (2): Linear(in_features=144, out_features=112, bias=True)\n",
       "    (3): CELU(alpha=0.1)\n",
       "    (4): Linear(in_features=112, out_features=96, bias=True)\n",
       "    (5): CELU(alpha=0.1)\n",
       "    (6): Linear(in_features=96, out_features=1, bias=True)\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=128, bias=True)\n",
       "    (1): CELU(alpha=0.1)\n",
       "    (2): Linear(in_features=128, out_features=112, bias=True)\n",
       "    (3): CELU(alpha=0.1)\n",
       "    (4): Linear(in_features=112, out_features=96, bias=True)\n",
       "    (5): CELU(alpha=0.1)\n",
       "    (6): Linear(in_features=96, out_features=1, bias=True)\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=128, bias=True)\n",
       "    (1): CELU(alpha=0.1)\n",
       "    (2): Linear(in_features=128, out_features=112, bias=True)\n",
       "    (3): CELU(alpha=0.1)\n",
       "    (4): Linear(in_features=112, out_features=96, bias=True)\n",
       "    (5): CELU(alpha=0.1)\n",
       "    (6): Linear(in_features=96, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_params(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, a=1.0)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "nn.apply(init_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcd0601",
   "metadata": {},
   "source": [
    "Let's now create a pipeline of AEV Computer --> Neural Networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1341cb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): AEVComputer()\n",
      "  (1): ANIModel(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=160, bias=True)\n",
      "      (1): CELU(alpha=0.1)\n",
      "      (2): Linear(in_features=160, out_features=128, bias=True)\n",
      "      (3): CELU(alpha=0.1)\n",
      "      (4): Linear(in_features=128, out_features=96, bias=True)\n",
      "      (5): CELU(alpha=0.1)\n",
      "      (6): Linear(in_features=96, out_features=1, bias=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=144, bias=True)\n",
      "      (1): CELU(alpha=0.1)\n",
      "      (2): Linear(in_features=144, out_features=112, bias=True)\n",
      "      (3): CELU(alpha=0.1)\n",
      "      (4): Linear(in_features=112, out_features=96, bias=True)\n",
      "      (5): CELU(alpha=0.1)\n",
      "      (6): Linear(in_features=96, out_features=1, bias=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "      (1): CELU(alpha=0.1)\n",
      "      (2): Linear(in_features=128, out_features=112, bias=True)\n",
      "      (3): CELU(alpha=0.1)\n",
      "      (4): Linear(in_features=112, out_features=96, bias=True)\n",
      "      (5): CELU(alpha=0.1)\n",
      "      (6): Linear(in_features=96, out_features=1, bias=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "      (1): CELU(alpha=0.1)\n",
      "      (2): Linear(in_features=128, out_features=112, bias=True)\n",
      "      (3): CELU(alpha=0.1)\n",
      "      (4): Linear(in_features=112, out_features=96, bias=True)\n",
      "      (5): CELU(alpha=0.1)\n",
      "      (6): Linear(in_features=96, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torchani.nn.Sequential(aev_computer, nn).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aa4350",
   "metadata": {},
   "source": [
    "Now let's setup the optimizers. NeuroChem uses Adam with decoupled weight decay\n",
    "to updates the weights and Stochastic Gradient Descent (SGD) to update the biases.\n",
    "Moreover, we need to specify different weight decay rate for different layes.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>The weight decay in `inputtrain.ipt`_ is named \"l2\", but it is actually not\n",
    "  L2 regularization. The confusion between L2 and weight decay is a common\n",
    "  mistake in deep learning.  See: `Decoupled Weight Decay Regularization`_\n",
    "  Also note that the weight decay only applies to weight in the training\n",
    "  of ANI models, not bias.</p></div>\n",
    "\n",
    "  https://arxiv.org/abs/1711.05101\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1395f981",
   "metadata": {},
   "outputs": [],
   "source": [
    "AdamW = torch.optim.AdamW([\n",
    "    #H networks\n",
    "    {'params': [H_network[0].weight]},\n",
    "    {'params': [H_network[2].weight], 'weight_decay': 0.00001},\n",
    "    {'params': [H_network[4].weight], 'weight_decay': 0.000001},\n",
    "    {'params': [H_network[6].weight]},\n",
    "    # C networks\n",
    "    {'params': [C_network[0].weight]},\n",
    "    {'params': [C_network[2].weight], 'weight_decay': 0.00001},\n",
    "    {'params': [C_network[4].weight], 'weight_decay': 0.000001},\n",
    "    {'params': [C_network[6].weight]},\n",
    "    # N networks\n",
    "    {'params': [N_network[0].weight]},\n",
    "    {'params': [N_network[2].weight], 'weight_decay': 0.00001},\n",
    "    {'params': [N_network[4].weight], 'weight_decay': 0.000001},\n",
    "    {'params': [N_network[6].weight]},\n",
    "    # O networks\n",
    "    {'params': [O_network[0].weight]},\n",
    "    {'params': [O_network[2].weight], 'weight_decay': 0.00001},\n",
    "    {'params': [O_network[4].weight], 'weight_decay': 0.000001},\n",
    "    {'params': [O_network[6].weight]},\n",
    "])\n",
    "\n",
    "SGD = torch.optim.SGD([\n",
    "    # H networks\n",
    "    {'params': [H_network[0].bias]},\n",
    "    {'params': [H_network[2].bias]},\n",
    "    {'params': [H_network[4].bias]},\n",
    "    {'params': [H_network[6].bias]},\n",
    "    # C networks\n",
    "    {'params': [C_network[0].bias]},\n",
    "    {'params': [C_network[2].bias]},\n",
    "    {'params': [C_network[4].bias]},\n",
    "    {'params': [C_network[6].bias]},\n",
    "    # N networks\n",
    "    {'params': [N_network[0].bias]},\n",
    "    {'params': [N_network[2].bias]},\n",
    "    {'params': [N_network[4].bias]},\n",
    "    {'params': [N_network[6].bias]},\n",
    "    # O networks\n",
    "    {'params': [O_network[0].bias]},\n",
    "    {'params': [O_network[2].bias]},\n",
    "    {'params': [O_network[4].bias]},\n",
    "    {'params': [O_network[6].bias]},\n",
    "], lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a0569a",
   "metadata": {},
   "source": [
    "Setting up a learning rate scheduler to do learning rate decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "902ff127",
   "metadata": {},
   "outputs": [],
   "source": [
    "AdamW_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(AdamW, factor=0.5, patience=100, threshold=0)\n",
    "SGD_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(SGD, factor=0.5, patience=100, threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ab28d",
   "metadata": {},
   "source": [
    "Train the model by minimizing the MSE loss, until validation RMSE no longer\n",
    "improves during a certain number of steps, decay the learning rate and repeat\n",
    "the same process, stop until the learning rate is smaller than a threshold.\n",
    "\n",
    "We first read the checkpoint files to restart training. We use `latest.pt`\n",
    "to store current training state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c9cbb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_checkpoint = 'latest.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b59833d",
   "metadata": {},
   "source": [
    "Resume training from previously saved checkpoints:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8556a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(latest_checkpoint):\n",
    "    checkpoint = torch.load(latest_checkpoint)\n",
    "    nn.load_state_dict(checkpoint['nn'])\n",
    "    AdamW.load_state_dict(checkpoint['AdamW'])\n",
    "    SGD.load_state_dict(checkpoint['SGD'])\n",
    "    AdamW_scheduler.load_state_dict(checkpoint['AdamW_scheduler'])\n",
    "    SGD_scheduler.load_state_dict(checkpoint['SGD_scheduler'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1b3bd4",
   "metadata": {},
   "source": [
    "During training, we need to validate on validation set and if validation error\n",
    "is better than the best, then save the new best model to a checkpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3efe83a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    # run validation\n",
    "    mse_sum = torch.nn.MSELoss(reduction='sum')\n",
    "    total_mse = 0.0\n",
    "    count = 0\n",
    "    for properties in validation:\n",
    "        species = properties['species'].to(device)\n",
    "        coordinates = properties['coordinates'].to(device).float()\n",
    "        true_energies = properties['energies'].to(device).float()\n",
    "        _, predicted_energies = model((species, coordinates))\n",
    "        total_mse += mse_sum(predicted_energies, true_energies).item()\n",
    "        count += predicted_energies.shape[0]\n",
    "    return hartree2kcalmol(math.sqrt(total_mse / count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc560692",
   "metadata": {},
   "source": [
    "We will also use TensorBoard to visualize our training process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bd8d9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = torch.utils.tensorboard.SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90e4a4b",
   "metadata": {},
   "source": [
    "Finally, we come to the training loop.\n",
    "\n",
    "In this tutorial, we are setting the maximum epoch to a very small number,\n",
    "only to make this demo terminate fast. For serious training, this should be\n",
    "set to a much larger value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d050a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torchani/aev.py:238: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  pair_sizes = counts * (counts - 1) // 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training starting from epoch 1\n",
      "RMSE: 48422.7185517338 at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100% 12/12 [00:00<00:00, 176.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 48346.813317694025 at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2: 100% 12/12 [00:00<00:00, 183.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 48067.281654420534 at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 3: 100% 12/12 [00:00<00:00, 196.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 47854.524102356845 at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4: 100% 12/12 [00:00<00:00, 187.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 46993.4338288407 at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5: 100% 12/12 [00:00<00:00, 182.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 46179.986073356245 at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6: 100% 12/12 [00:00<00:00, 180.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 47473.185005905194 at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 7: 100% 12/12 [00:00<00:00, 164.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 47080.49889416281 at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 8: 100% 12/12 [00:00<00:00, 156.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 47526.93207025697 at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 9: 100% 12/12 [00:00<00:00, 184.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 48041.753898674586 at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 10: 100% 12/12 [00:00<00:00, 199.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 44127.79949721311 at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 11: 100% 12/12 [00:00<00:00, 197.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 41498.57346366373 at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 12: 100% 12/12 [00:00<00:00, 174.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 50478.326630078554 at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 13: 100% 12/12 [00:00<00:00, 177.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 43134.821973293765 at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 14: 100% 12/12 [00:00<00:00, 188.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 46558.22998646119 at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 15: 100% 12/12 [00:00<00:00, 176.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 42721.67738337035 at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 16: 100% 12/12 [00:00<00:00, 166.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 39292.62726430657 at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 17: 100% 12/12 [00:00<00:00, 144.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 53969.56391522538 at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 18: 100% 12/12 [00:00<00:00, 142.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 54658.04044956212 at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 19: 100% 12/12 [00:00<00:00, 176.67it/s]\n"
     ]
    }
   ],
   "source": [
    "mse = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "print(\"training starting from epoch\", AdamW_scheduler.last_epoch + 1)\n",
    "max_epochs = 20\n",
    "early_stopping_learning_rate = 1.0E-5\n",
    "best_model_checkpoint = 'best.pt'\n",
    "\n",
    "for _ in range(AdamW_scheduler.last_epoch + 1, max_epochs):\n",
    "    rmse = validate()\n",
    "    #rmse =  0\n",
    "    print('RMSE:', rmse, 'at epoch', AdamW_scheduler.last_epoch + 1)\n",
    "\n",
    "    learning_rate = AdamW.param_groups[0]['lr']\n",
    "\n",
    "    if learning_rate < early_stopping_learning_rate:\n",
    "        break\n",
    "\n",
    "    # checkpoint\n",
    "    if AdamW_scheduler.is_better(rmse, AdamW_scheduler.best):\n",
    "        torch.save(nn.state_dict(), best_model_checkpoint)\n",
    "\n",
    "    AdamW_scheduler.step(rmse)\n",
    "    SGD_scheduler.step(rmse)\n",
    "\n",
    "    tensorboard.add_scalar('validation_rmse', rmse, AdamW_scheduler.last_epoch)\n",
    "    tensorboard.add_scalar('best_validation_rmse', AdamW_scheduler.best, AdamW_scheduler.last_epoch)\n",
    "    tensorboard.add_scalar('learning_rate', learning_rate, AdamW_scheduler.last_epoch)\n",
    "\n",
    "    for i, properties in tqdm.tqdm(\n",
    "        enumerate(training),\n",
    "        total=len(training),\n",
    "        desc=\"epoch {}\".format(AdamW_scheduler.last_epoch)\n",
    "    ):\n",
    "        species = properties['species'].to(device)\n",
    "        coordinates = properties['coordinates'].to(device).float()\n",
    "        true_energies = properties['energies'].to(device).float()\n",
    "        num_atoms = (species >= 0).sum(dim=1, dtype=true_energies.dtype)\n",
    "        _, predicted_energies = model((species, coordinates))\n",
    "\n",
    "        loss = (mse(predicted_energies, true_energies) / num_atoms.sqrt()).mean()\n",
    "\n",
    "        AdamW.zero_grad()\n",
    "        SGD.zero_grad()\n",
    "        loss.backward()\n",
    "        AdamW.step()\n",
    "        SGD.step()\n",
    "\n",
    "        # write current batch loss to TensorBoard\n",
    "        tensorboard.add_scalar('batch_loss', loss, AdamW_scheduler.last_epoch * len(training) + i)\n",
    "\n",
    "    torch.save({\n",
    "        'nn': nn.state_dict(),\n",
    "        'AdamW': AdamW.state_dict(),\n",
    "        'SGD': SGD.state_dict(),\n",
    "        'AdamW_scheduler': AdamW_scheduler.state_dict(),\n",
    "        'SGD_scheduler': SGD_scheduler.state_dict(),\n",
    "    }, latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf04ba5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): AEVComputer()\n",
       "  (1): ANIModel(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=160, bias=True)\n",
       "      (1): CELU(alpha=0.1)\n",
       "      (2): Linear(in_features=160, out_features=128, bias=True)\n",
       "      (3): CELU(alpha=0.1)\n",
       "      (4): Linear(in_features=128, out_features=96, bias=True)\n",
       "      (5): CELU(alpha=0.1)\n",
       "      (6): Linear(in_features=96, out_features=1, bias=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=144, bias=True)\n",
       "      (1): CELU(alpha=0.1)\n",
       "      (2): Linear(in_features=144, out_features=112, bias=True)\n",
       "      (3): CELU(alpha=0.1)\n",
       "      (4): Linear(in_features=112, out_features=96, bias=True)\n",
       "      (5): CELU(alpha=0.1)\n",
       "      (6): Linear(in_features=96, out_features=1, bias=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=128, bias=True)\n",
       "      (1): CELU(alpha=0.1)\n",
       "      (2): Linear(in_features=128, out_features=112, bias=True)\n",
       "      (3): CELU(alpha=0.1)\n",
       "      (4): Linear(in_features=112, out_features=96, bias=True)\n",
       "      (5): CELU(alpha=0.1)\n",
       "      (6): Linear(in_features=96, out_features=1, bias=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=128, bias=True)\n",
       "      (1): CELU(alpha=0.1)\n",
       "      (2): Linear(in_features=128, out_features=112, bias=True)\n",
       "      (3): CELU(alpha=0.1)\n",
       "      (4): Linear(in_features=112, out_features=96, bias=True)\n",
       "      (5): CELU(alpha=0.1)\n",
       "      (6): Linear(in_features=96, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model, 'save_model_1.pt')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c734590a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
